{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Utility\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "D:\\Software\\Utility\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "##stuff to download to get it to work\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download fr\n",
    "#wget https://s3.amazonaws.com/opennmt-models/iwslt.pt\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data#, datasets\n",
    "\n",
    "import transformer\n",
    "import dataLoaderIWLST \n",
    "\n",
    "##visualization thingy\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "#%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available cuda devices :  1\n"
     ]
    }
   ],
   "source": [
    "#create table of different GPUSs\n",
    "#print(\"current device index\", torch.cuda.current_device())\n",
    "#current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"number of available cuda devices : \", torch.cuda.device_count())\n",
    "devices = range(torch.cuda.device_count())\n",
    "\n",
    "#parameters\n",
    "justEvaluate = False\n",
    "loadPreTrain = False\n",
    "trainItNb = 10\n",
    "BATCH_SIZE = 12000\n",
    "validFreq = 5\n",
    "previousEpochNb = 0\n",
    "modelSavePath = \"Model/modelIWSLT.nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return transformer.Batch(src, trg, pad_idx)\n",
    "\n",
    "class MultiGPULossCompute:\n",
    "    \"A multi-gpu loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
    "        # Send out to different gpus.\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate(criterion, \n",
    "                                               devices=devices)\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "    def __call__(self, out, targets, normalize):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate(self.generator, \n",
    "                                                devices=self.devices)\n",
    "        out_scatter = nn.parallel.scatter(out, \n",
    "                                          target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        targets = nn.parallel.scatter(targets, \n",
    "                                      target_gpus=self.devices)\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            # Predict distributions\n",
    "            out_column = [[Variable(o[:, i:i+chunk_size].data, \n",
    "                                    requires_grad=self.opt is not None)] \n",
    "                           for o in out_scatter]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "\n",
    "            # Compute loss. \n",
    "            y = [(g.contiguous().view(-1, g.size(-1)), \n",
    "                  t[:, i:i+chunk_size].contiguous().view(-1)) \n",
    "                 for g, t in zip(gen, targets)]\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l = nn.parallel.gather(loss, \n",
    "                                   target_device=self.devices[0])\n",
    "            l = l.sum()[0] / normalize\n",
    "            total += l.data[0]\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "\n",
    "        # Backprop all loss through transformer.            \n",
    "        if self.opt is not None:\n",
    "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather(out_grad, \n",
    "                                    target_device=self.devices[0])\n",
    "            o1.backward(gradient=o2)\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return total * normalize\n",
    "\n",
    "def training(model, optimizer, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, devices) :\n",
    "    for epoch in range(trainItNb):\n",
    "        model.train()\n",
    "        #run_epoch computes the loss given the input optimizer function, which is here MultiGPULossCompute\n",
    "        #if the argument optimizer of MultiGPULossCompute isn't none then it will also perform the backprop\n",
    "        transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
    "                  model, \n",
    "                  MultiGPULossCompute(model.generator, criterion, \n",
    "                                      devices=devices, opt=optimizer))\n",
    "        #validation\n",
    "        if (epoch % validFreq == 0):\n",
    "            evaluate(model, valid_iter, criterion, devices, optimizer, pad_idx)\n",
    "\n",
    "def evaluate(model, valid_iter, criterion, devices, optimizer, pad_idx) :\n",
    "    model.eval()\n",
    "    loss = transformer.run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
    "                     model, \n",
    "                     MultiGPULossCompute(model.generator, criterion, \n",
    "                     devices=devices, opt=None))\n",
    "    print(\"loss : \", loss)\n",
    "\n",
    "def saveModel(model, epoch, optimizer, batchSize, PATH):\n",
    "    state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'batchSize' : batchSize\n",
    "    }\n",
    "    torch.save(state, PATH)\n",
    "\n",
    "def loadModel(PATH, SRC, TGT):\n",
    "    state = torch.load(PATH)\n",
    "    \n",
    "    model = transformer.make_model(len(SRC.vocab), len(TGT.vocab))\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    \n",
    "    optimizer = transformer.NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))   \n",
    "    optimizer.load_state_dict(state['optimizer']) \n",
    "    \n",
    "    batchSize = 0\n",
    "    batchSize.load_state_dict(state['batchSize'])\n",
    "    \n",
    "    epoch = 0\n",
    "    epoch.load_state_dict(state['epoch'])\n",
    "\n",
    "    return model,optimizer,batchSize,epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Doing spacy load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Utility\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "D:\\Software\\Utility\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy load completed\n",
      "Creating data Fields\n",
      "Splitting\n",
      "downloading fr-en.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\iwslt\\fr-en.tgz: 100%|██████████████████████████████████████████████████████| 25.7M/25.7M [00:14<00:00, 1.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".data\\iwslt\\fr-en\\IWSLT16.TED.dev2010.fr-en.en.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.dev2010.fr-en.fr.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2010.fr-en.en.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2010.fr-en.fr.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2011.fr-en.en.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2011.fr-en.fr.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2012.fr-en.en.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2012.fr-en.fr.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2013.fr-en.en.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2013.fr-en.fr.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2014.fr-en.en.xml\n",
      ".data\\iwslt\\fr-en\\IWSLT16.TED.tst2014.fr-en.fr.xml\n",
      ".data\\iwslt\\fr-en\\train.tags.fr-en.en\n",
      ".data\\iwslt\\fr-en\\train.tags.fr-en.fr\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "print(\"Loading Data\")\n",
    "SRC,TGT,train,val,test, pad_idx = dataLoaderIWLST.loadDataIWLST()\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (loadPreTrain or justEvaluate) :\n",
    "    print(\"Loading pre-trained network\")\n",
    "    model, model_opt, BATCH_SIZE, previousEpochNb = loadModel(modelSavePath, SRC, TGT)\n",
    "else :\n",
    "    print(\"initializing network\")\n",
    "    model = transformer.make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "    model_opt = transformer.NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    \n",
    "#model.to(current_device)\n",
    "model.cuda()\n",
    "criterion = transformer.LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "#criterion.to(current_device)\n",
    "\n",
    "print(\"Initializing iterators\")\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device = 0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=transformer.batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device = 0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=transformer.batch_size_fn, train=False)\n",
    "\n",
    "#if more than one GPU, go parallel\n",
    "model_par = nn.DataParallel(model, device_ids = devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (justEvaluate) :\n",
    "    print(\"Starting training\")\n",
    "    training(model_par, model_opt, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, devices)\n",
    "    evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)\n",
    "\n",
    "    print(\"Saving network\")\n",
    "    saveModel(model_par, previousEpochNb + trainItNb, model_opt, BATCH_SIZE, modelSavePath)\n",
    "else :\n",
    "    evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
