{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 943
    },
    "colab_type": "code",
    "id": "t7xPPf3u6mex",
    "outputId": "f855a133-83b4-4305-ccd0-a15d1b706b93"
   },
   "outputs": [],
   "source": [
    "##stuff to download to get it to work\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download fr\n",
    "#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data#, datasets\n",
    "\n",
    "import transformer\n",
    "import dataLoaderIWLST \n",
    "\n",
    "##visualization thingy\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "#%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Zwv4jfka6mfC",
    "outputId": "14fa8741-f393-43f5-c882-aa6c2604f22f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING : As of now MultiGpu is not supported so all options using MultiGPU will be downgraded to single GPU\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#create table of different GPUSs\n",
    "#print(\"current device index\", torch.cuda.current_device())\n",
    "#current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#print(\"number of available cuda devices : \", torch.cuda.device_count())\n",
    "#devices = range(torch.cuda.device_count())\n",
    "print(\"WARNING : As of now MultiGpu is not supported so all options using MultiGPU will be downgraded to single GPU\")\n",
    "#device = torch.cuda.current_device() \n",
    "device = torch.device(torch.cuda.current_device())\n",
    "print(device)\n",
    "#parameters\n",
    "justEvaluate = False\n",
    "loadPreTrain = False\n",
    "trainItNb = 10\n",
    "BATCH_SIZE = 10\n",
    "validFreq = 5\n",
    "previousEpochNb = 0\n",
    "modelSavePath = \"Model/modelIWSLT.nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "QnczHW4g6mfZ",
    "outputId": "4df3fd97-9a98-4e68-af01-f857743ef914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Doing spacy load\n",
      "Spacy load completed\n",
      "Creating data Fields\n",
      "Splitting\n",
      "Building vocabulary\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "print(\"Loading Data\")\n",
    "SRC,TGT,train,val,test, pad_idx = dataLoaderIWLST.loadDataIWLST()\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Yj53F5A6mfs"
   },
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return transformer.Batch(src, trg, pad_idx)\n",
    "\n",
    "class MultiGPULossCompute:\n",
    "    \"A multi-gpu loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
    "        # Send out to different gpus.\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate(criterion, \n",
    "                                               devices=devices)\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "    def __call__(self, out, targets, normalize):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate(self.generator, \n",
    "                                                devices=self.devices)\n",
    "        out_scatter = nn.parallel.scatter(out, \n",
    "                                          target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        targets = nn.parallel.scatter(targets, \n",
    "                                      target_gpus=self.devices)\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            # Predict distributions\n",
    "            out_column = [[Variable(o[:, i:i+chunk_size].data, \n",
    "                                    requires_grad=self.opt is not None)] \n",
    "                           for o in out_scatter]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "\n",
    "            # Compute loss. \n",
    "            y = [(g.contiguous().view(-1, g.size(-1)), \n",
    "                  t[:, i:i+chunk_size].contiguous().view(-1)) \n",
    "                 for g, t in zip(gen, targets)]\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l = nn.parallel.gather(loss, \n",
    "                                   target_device=self.devices[0])\n",
    "            l = l.sum()[0] / normalize\n",
    "            total += l.data[0]\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "\n",
    "        # Backprop all loss through transformer.            \n",
    "        if self.opt is not None:\n",
    "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather(out_grad, \n",
    "                                    target_device=self.devices[0])\n",
    "            o1.backward(gradient=o2)\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return total * normalize\n",
    "\n",
    "class SingleGPULossCompute:\n",
    "    \"A single-gpu loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, device, opt=None, chunk_size=5):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        self.device = device\n",
    "        #self.chunk_size = chunk_size\n",
    "        \n",
    "    def __call__(self, out, targets, normalize):\n",
    "        \n",
    "        #out = out.to_device(self.device)\n",
    "        #targets= targets.to_device(self.device)\n",
    "        \n",
    "        out = out.to(device)\n",
    "        targets = targets.long().to(device)\n",
    "        #apply generator\n",
    "        gen = self.generator.forward(out).to(device)\n",
    "\n",
    "        #compute loss by applying criterion\n",
    "          \n",
    "        \n",
    "        loss = self.criterion.forward(gen.contiguous().view(-1, gen.size(-1)), targets.contiguous().view(-1, gen.size(-1)))\n",
    "        #backprop to transformer output\n",
    "        loss.backward()\n",
    "        #backprop through transformer\n",
    "        grad1 = out.grad.data.clone()\n",
    "        out.backward(gradient = grad1)\n",
    "        #step optimizer and zero_grad()\n",
    "        self.opt.step()\n",
    "        self.opt.optimizer.zero_grad()\n",
    "        \n",
    "        return loss      \n",
    "    \n",
    "def training(model, optimizer, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, device) :\n",
    "    for epoch in range(trainItNb):\n",
    "        model.train()\n",
    "        #run_epoch computes the loss given the input optimizer function, which is here MultiGPULossCompute\n",
    "        #if the argument optimizer of MultiGPULossCompute isn't none then it will also perform the backprop\n",
    "        transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
    "                  model, \n",
    "                  SingleGPULossCompute(model.generator, criterion, \n",
    "                                      device=device, opt=optimizer))\n",
    "        #transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
    "        #          model, \n",
    "        #          MultiGPULossCompute(model.generator, criterion, \n",
    "        #                              devices=devices, opt=optimizer))\n",
    "        \n",
    "        #validation\n",
    "        if (epoch % validFreq == 0):\n",
    "            evaluate(model, valid_iter, criterion, device, optimizer, pad_idx)\n",
    "\n",
    "def evaluate(model, valid_iter, criterion, device, optimizer, pad_idx) :\n",
    "    model.eval()\n",
    "    loss = transformer.run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
    "                     model, \n",
    "                     SingleGPULossCompute(model.generator, criterion, \n",
    "                     device=device, opt=None))\n",
    "    #loss = transformer.run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
    "    #                 model, \n",
    "    #                 MultiGPULossCompute(model.generator, criterion, \n",
    "    #                 devices=devices, opt=None))\n",
    "    \n",
    "    print(\"loss : \", loss)\n",
    "\n",
    "def saveModel(model, epoch, optimizer, batchSize, PATH):\n",
    "    state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'batchSize' : batchSize\n",
    "    }\n",
    "    torch.save(state, PATH)\n",
    "\n",
    "def loadModel(PATH, SRC, TGT):\n",
    "    state = torch.load(PATH)\n",
    "    \n",
    "    model = transformer.make_model(len(SRC.vocab), len(TGT.vocab))\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    \n",
    "    optimizer = transformer.NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))   \n",
    "    optimizer.load_state_dict(state['optimizer']) \n",
    "    \n",
    "    batchSize = 0\n",
    "    batchSize.load_state_dict(state['batchSize'])\n",
    "    \n",
    "    epoch = 0\n",
    "    epoch.load_state_dict(state['epoch'])\n",
    "\n",
    "    return model,optimizer,batchSize,epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "eBVvLZTl6mf3",
    "outputId": "7a6a4ea2-b30e-47a2-cf74-c33f060112ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niels\\DeepLearningProject\\transformer.py:251: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n",
      "D:\\Software\\Utility\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing iterators\n"
     ]
    }
   ],
   "source": [
    "if (loadPreTrain or justEvaluate) :\n",
    "    print(\"Loading pre-trained network\")\n",
    "    model, model_opt, BATCH_SIZE, previousEpochNb = loadModel(modelSavePath, SRC, TGT)\n",
    "else :\n",
    "    print(\"initializing network\")\n",
    "    model = transformer.make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "    model_opt = transformer.NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    \n",
    "model.cuda()\n",
    "criterion = transformer.LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Initializing iterators\")\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device = device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=transformer.batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device = device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=transformer.batch_size_fn, train=False)\n",
    "\n",
    "#if more than one GPU, go parallel\n",
    "#model_par = nn.DataParallel(model, device_ids = devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1912
    },
    "colab_type": "code",
    "id": "yzlmVC2z6mgH",
    "outputId": "4ec83ba1-cc36-49df-fa5a-504ccdac8cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[-1 x 38627]' is invalid for input with 14 elements at ..\\aten\\src\\TH\\THStorage.cpp:80",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-143ee3d0be11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainItNb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidFreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-ee9e19606420>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, optimizer, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, device)\u001b[0m\n\u001b[0;32m    123\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                   SingleGPULossCompute(model.generator, criterion, \n\u001b[1;32m--> 125\u001b[1;33m                                       device=device, opt=optimizer))\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[1;31m#transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m#          model,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DeepLearningProject\\transformer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[0;32m    304\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[0;32m    305\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-ee9e19606420>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, out, targets, normalize)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;31m#backprop to transformer output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[-1 x 38627]' is invalid for input with 14 elements at ..\\aten\\src\\TH\\THStorage.cpp:80"
     ]
    }
   ],
   "source": [
    "if not (justEvaluate) :\n",
    "    print(\"Starting training\")\n",
    "    #training(model_par, model_opt, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, devices)\n",
    "    #evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)\n",
    "   \n",
    "    training(model, model_opt, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, device)\n",
    "    evaluate(model, valid_iter, criterion, device, model_opt, pad_idx)\n",
    "\n",
    "    print(\"Saving network\")\n",
    "    #saveModel(model_par, previousEpochNb + trainItNb, model_opt, BATCH_SIZE, modelSavePath)\n",
    "    saveModel(model, previousEpochNb + trainItNb, model_opt, BATCH_SIZE, modelSavePath)\n",
    "else :\n",
    "    evaluate(model, valid_iter, criterion, device, model_opt, pad_idx)\n",
    "    #evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
