{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "t7xPPf3u6mex",
        "colab_type": "code",
        "outputId": "afc35e21-d2f8-4fbd-9952-06e689fcb4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "cell_type": "code",
      "source": [
        "##stuff to download to get it to work\n",
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download fr\n",
        "#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt\n",
        "\n",
        "\n",
        "#import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "#import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "from torchtext import data#, datasets\n",
        "\n",
        "import transformer\n",
        "import dataLoaderIWLST \n",
        "\n",
        "##visualization thingy\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn\n",
        "#seaborn.set_context(context=\"talk\")\n",
        "#%matplotlib inline\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.14.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (0.4.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.3.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.10.15)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy<0.4.4->spacy) (0.5.6)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.11)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.11.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: fr_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.0.0/fr_core_news_sm-2.0.0.tar.gz#egg=fr_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "\n",
            "    You can now load the model via spacy.load('fr')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1PxysiJt67B3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zwv4jfka6mfC",
        "colab_type": "code",
        "outputId": "bc3e3f8d-e052-485d-89d8-6afc07377538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#create table of different GPUSs\n",
        "#print(\"current device index\", torch.cuda.current_device())\n",
        "#current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#print(\"number of available cuda devices : \", torch.cuda.device_count())\n",
        "#devices = range(torch.cuda.device_count())\n",
        "print(\"WARNING : As of now MultiGpu is not supported so all options using MultiGPU will be downgraded to single GPU\")\n",
        "#device = torch.cuda.current_device() \n",
        "device = torch.device(torch.cuda.current_device())\n",
        "print(device)\n",
        "#parameters\n",
        "justEvaluate = False\n",
        "loadPreTrain = False\n",
        "trainItNb = 10\n",
        "BATCH_SIZE = 20\n",
        "validFreq = 5\n",
        "previousEpochNb = 0\n",
        "modelSavePath = \"Model/modelIWSLT.nn\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING : As of now MultiGpu is not supported so all options using MultiGPU will be downgraded to single GPU\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QnczHW4g6mfZ",
        "colab_type": "code",
        "outputId": "d12fe211-3fea-4bb2-f81f-9a219c3ba27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "#Load Data\n",
        "print(\"Loading Data\")\n",
        "SRC,TGT,train,val,test, pad_idx = dataLoaderIWLST.loadDataIWLST()\n",
        "print(\"Data Loaded\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data\n",
            "Doing spacy load\n",
            "Spacy load completed\n",
            "Creating data Fields\n",
            "Splitting\n",
            "Building vocabulary\n",
            "Data Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Yj53F5A6mfs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return transformer.Batch(src, trg, pad_idx)\n",
        "\n",
        "class MultiGPULossCompute:\n",
        "    \"A multi-gpu loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
        "        # Send out to different gpus.\n",
        "        self.generator = generator\n",
        "        self.criterion = nn.parallel.replicate(criterion, \n",
        "                                               devices=devices)\n",
        "        self.opt = opt\n",
        "        self.devices = devices\n",
        "        self.chunk_size = chunk_size\n",
        "        \n",
        "    def __call__(self, out, targets, normalize):\n",
        "        total = 0.0\n",
        "        generator = nn.parallel.replicate(self.generator, \n",
        "                                                devices=self.devices)\n",
        "        out_scatter = nn.parallel.scatter(out, \n",
        "                                          target_gpus=self.devices)\n",
        "        out_grad = [[] for _ in out_scatter]\n",
        "        targets = nn.parallel.scatter(targets, \n",
        "                                      target_gpus=self.devices)\n",
        "\n",
        "        # Divide generating into chunks.\n",
        "        chunk_size = self.chunk_size\n",
        "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
        "            # Predict distributions\n",
        "            out_column = [[Variable(o[:, i:i+chunk_size].data, \n",
        "                                    requires_grad=self.opt is not None)] \n",
        "                           for o in out_scatter]\n",
        "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
        "\n",
        "            # Compute loss. \n",
        "            y = [(g.contiguous().view(-1, g.size(-1)), \n",
        "                  t[:, i:i+chunk_size].contiguous().view(-1)) \n",
        "                 for g, t in zip(gen, targets)]\n",
        "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
        "\n",
        "            # Sum and normalize loss\n",
        "            l = nn.parallel.gather(loss, \n",
        "                                   target_device=self.devices[0])\n",
        "            l = l.sum()[0] / normalize\n",
        "            total += l.data[0]\n",
        "\n",
        "            # Backprop loss to output of transformer\n",
        "            if self.opt is not None:\n",
        "                l.backward()\n",
        "                for j, l in enumerate(loss):\n",
        "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
        "\n",
        "        # Backprop all loss through transformer.            \n",
        "        if self.opt is not None:\n",
        "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
        "            o1 = out\n",
        "            o2 = nn.parallel.gather(out_grad, \n",
        "                                    target_device=self.devices[0])\n",
        "            o1.backward(gradient=o2)\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return total * normalize\n",
        "\n",
        "class SingleGPULossCompute:\n",
        "    \"A single-gpu loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, device, opt=None, chunk_size=5):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        self.device = device\n",
        "        #self.chunk_size = chunk_size\n",
        "        \n",
        "    def __call__(self, out, targets, normalize):\n",
        "        \n",
        "        #out = out.to_device(self.device)\n",
        "        #targets= targets.to_device(self.device)\n",
        "        \n",
        "        out = out.to(device)\n",
        "        targets = targets.long().to(device)\n",
        "        #apply generator\n",
        "        gen = self.generator.forward(out)\n",
        "        #compute loss by applying criterion\n",
        "        y = torch.cat(torch.FloatTensor(gen.view(-1, gen.size(-1)), targets.contiguous().view(-1)))\n",
        "        torch.stack(y)\n",
        "        loss = self.criterion(y, targets)\n",
        "        #backprop to transformer output\n",
        "        loss.backward()\n",
        "        #backprop through transformer\n",
        "        grad1 = out.grad.data.clone()\n",
        "        out.backward(gradient = grad1)\n",
        "        #step optimizer and zero_grad()\n",
        "        self.opt.step()\n",
        "        self.opt.optimizer.zero_grad()\n",
        "        \n",
        "        return loss      \n",
        "    \n",
        "def training(model, optimizer, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, device) :\n",
        "    for epoch in range(trainItNb):\n",
        "        model.train()\n",
        "        #run_epoch computes the loss given the input optimizer function, which is here MultiGPULossCompute\n",
        "        #if the argument optimizer of MultiGPULossCompute isn't none then it will also perform the backprop\n",
        "        transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model, \n",
        "                  SingleGPULossCompute(model.generator, criterion, \n",
        "                                      device=device, opt=optimizer))\n",
        "        #transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "        #          model, \n",
        "        #          MultiGPULossCompute(model.generator, criterion, \n",
        "        #                              devices=devices, opt=optimizer))\n",
        "        \n",
        "        #validation\n",
        "        if (epoch % validFreq == 0):\n",
        "            evaluate(model, valid_iter, criterion, device, optimizer, pad_idx)\n",
        "\n",
        "def evaluate(model, valid_iter, criterion, device, optimizer, pad_idx) :\n",
        "    model.eval()\n",
        "    loss = transformer.run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                     model, \n",
        "                     SingleGPULossCompute(model.generator, criterion, \n",
        "                     device=device, opt=None))\n",
        "    #loss = transformer.run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "    #                 model, \n",
        "    #                 MultiGPULossCompute(model.generator, criterion, \n",
        "    #                 devices=devices, opt=None))\n",
        "    \n",
        "    print(\"loss : \", loss)\n",
        "\n",
        "def saveModel(model, epoch, optimizer, batchSize, PATH):\n",
        "    state = {\n",
        "    'epoch': epoch,\n",
        "    'state_dict': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'batchSize' : batchSize\n",
        "    }\n",
        "    torch.save(state, PATH)\n",
        "\n",
        "def loadModel(PATH, SRC, TGT):\n",
        "    state = torch.load(PATH)\n",
        "    \n",
        "    model = transformer.make_model(len(SRC.vocab), len(TGT.vocab))\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    \n",
        "    optimizer = transformer.NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))   \n",
        "    optimizer.load_state_dict(state['optimizer']) \n",
        "    \n",
        "    batchSize = 0\n",
        "    batchSize.load_state_dict(state['batchSize'])\n",
        "    \n",
        "    epoch = 0\n",
        "    epoch.load_state_dict(state['epoch'])\n",
        "\n",
        "    return model,optimizer,batchSize,epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBVvLZTl6mf3",
        "colab_type": "code",
        "outputId": "6283e8b7-4795-4788-c4bc-a8a08e49e58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "if (loadPreTrain or justEvaluate) :\n",
        "    print(\"Loading pre-trained network\")\n",
        "    model, model_opt, BATCH_SIZE, previousEpochNb = loadModel(modelSavePath, SRC, TGT)\n",
        "else :\n",
        "    print(\"initializing network\")\n",
        "    model = transformer.make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "    model_opt = transformer.NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "model.cuda()\n",
        "criterion = transformer.LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Initializing iterators\")\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device = device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=transformer.batch_size_fn, train=True)\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device = device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=transformer.batch_size_fn, train=False)\n",
        "\n",
        "#if more than one GPU, go parallel\n",
        "#model_par = nn.DataParallel(model, device_ids = devices)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initializing network\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/transformer.py:251: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initializing iterators\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yzlmVC2z6mgH",
        "colab_type": "code",
        "outputId": "d07ce146-1775-4d73-da81-4aa4c18a72e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "cell_type": "code",
      "source": [
        "if not (justEvaluate) :\n",
        "    print(\"Starting training\")\n",
        "    #training(model_par, model_opt, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, devices)\n",
        "    #evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)\n",
        "    %pdb off\n",
        "    training(model, model_opt, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, device)\n",
        "    evaluate(model, valid_iter, criterion, device, model_opt, pad_idx)\n",
        "\n",
        "    print(\"Saving network\")\n",
        "    #saveModel(model_par, previousEpochNb + trainItNb, model_opt, BATCH_SIZE, modelSavePath)\n",
        "    saveModel(model, previousEpochNb + trainItNb, model_opt, BATCH_SIZE, modelSavePath)\n",
        "else :\n",
        "    evaluate(model, valid_iter, criterion, device, model_opt, pad_idx)\n",
        "    #evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training\n",
            "Automatic pdb calling has been turned OFF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0c7ddab1dcfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#evaluate(model_par, valid_iter, criterion, devices, model_opt, pad_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pdb off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainItNb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidFreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b34e562c18e3>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, optimizer, trainItNb, train_iter, valid_iter, validFreq, criterion, pad_idx, device)\u001b[0m\n\u001b[1;32m    122\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                   SingleGPULossCompute(model.generator, criterion, \n\u001b[0;32m--> 124\u001b[0;31m                                       device=device, opt=optimizer))\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;31m#transformer.run_epoch((rebatch(pad_idx, b) for b in train_iter),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m#          model,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m    304\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m    305\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b34e562c18e3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out, targets, normalize)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#compute loss by applying criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (!Tensor!, !Tensor!)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (!Tensor!, !Tensor!)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uTi68Lml6mgf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}